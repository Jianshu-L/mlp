{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True):\n",
    "    \n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 10102016 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 50\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = MNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = MNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, LeakyReluLayer, ELULayer, SELULayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit, SELUInit\n",
    "from mlp.learning_rules import GradientDescentLearningRule\n",
    "from mlp.optimisers import Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers Sigmoid Layer\n",
    "learning_rate = 0.1\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    SigmoidLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    SigmoidLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "Sigmoid_Layer = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)   \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(Sigmoid_Layer[0][-1, Sigmoid_Layer[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(Sigmoid_Layer[0][-1, Sigmoid_Layer[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(Sigmoid_Layer[0][-1, Sigmoid_Layer[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(Sigmoid_Layer[0][-1, Sigmoid_Layer[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(Sigmoid_Layer[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(Sigmoid_Layer[0][-1, Sigmoid_Layer[1]['error(train)']])\n",
    "final_errors_valid.append(Sigmoid_Layer[0][-1, Sigmoid_Layer[1]['error(valid)']])\n",
    "final_accs_train.append(Sigmoid_Layer[0][-1, Sigmoid_Layer[1]['acc(train)']])\n",
    "final_accs_valid.append(Sigmoid_Layer[0][-1, Sigmoid_Layer[1]['acc(valid)']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers Relu Layer\n",
    "learning_rate = 0.1\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "Relu_Layer = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(Relu_Layer[0][-1, Relu_Layer[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(Relu_Layer[0][-1, Relu_Layer[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(Relu_Layer[0][-1, Relu_Layer[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(Relu_Layer[0][-1, Relu_Layer[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(Relu_Layer[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(Relu_Layer[0][-1, Relu_Layer[1]['error(train)']])\n",
    "final_errors_valid.append(Relu_Layer[0][-1, Relu_Layer[1]['error(valid)']])\n",
    "final_accs_train.append(Relu_Layer[0][-1, Relu_Layer[1]['acc(train)']])\n",
    "final_accs_valid.append(Relu_Layer[0][-1, Relu_Layer[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers LeakyRelu Layer\n",
    "learning_rate = 0.1\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "Leaky_ReluLayer = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(Leaky_ReluLayer[0][-1, Leaky_ReluLayer[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(Leaky_ReluLayer[0][-1, Leaky_ReluLayer[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(Leaky_ReluLayer[0][-1, Leaky_ReluLayer[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(Leaky_ReluLayer[0][-1, Leaky_ReluLayer[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(Leaky_ReluLayer[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(Leaky_ReluLayer[0][-1, Leaky_ReluLayer[1]['error(train)']])\n",
    "final_errors_valid.append(Leaky_ReluLayer[0][-1, Leaky_ReluLayer[1]['error(valid)']])\n",
    "final_accs_train.append(Leaky_ReluLayer[0][-1, Leaky_ReluLayer[1]['acc(train)']])\n",
    "final_accs_valid.append(Leaky_ReluLayer[0][-1, Leaky_ReluLayer[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers ELU Layer\n",
    "learning_rate = 0.1\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    ELULayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ELULayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "ELU_Layer = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)   \n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(ELU_Layer[0][-1, ELU_Layer[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(ELU_Layer[0][-1, ELU_Layer[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(ELU_Layer[0][-1, ELU_Layer[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(ELU_Layer[0][-1, ELU_Layer[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(ELU_Layer[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(ELU_Layer[0][-1, ELU_Layer[1]['error(train)']])\n",
    "final_errors_valid.append(ELU_Layer[0][-1, ELU_Layer[1]['error(valid)']])\n",
    "final_accs_train.append(ELU_Layer[0][-1, ELU_Layer[1]['acc(train)']])\n",
    "final_accs_valid.append(ELU_Layer[0][-1, ELU_Layer[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers SELU Layer\n",
    "learning_rate = 0.1\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "SELU_Layer = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(SELU_Layer[0][-1, SELU_Layer[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(SELU_Layer[0][-1, SELU_Layer[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(SELU_Layer[0][-1, SELU_Layer[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(SELU_Layer[0][-1, SELU_Layer[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(SELU_Layer[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(SELU_Layer[0][-1, SELU_Layer[1]['error(train)']])\n",
    "final_errors_valid.append(SELU_Layer[0][-1, SELU_Layer[1]['error(valid)']])\n",
    "final_accs_train.append(SELU_Layer[0][-1, SELU_Layer[1]['acc(train)']])\n",
    "final_accs_valid.append(SELU_Layer[0][-1, SELU_Layer[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [Sigmoid_Layer, Relu_Layer, Leaky_ReluLayer, \n",
    "          ELU_Layer, SELU_Layer]\n",
    "models_name = ['SigmoidLayer', 'ReluLayer', 'LeakyReluLayer', \n",
    "               'ELULayer', 'SELULayer']\n",
    "fig_1 = plt.figure(figsize=(10, 12))\n",
    "ax_1 = fig_1.add_subplot(211)\n",
    "for model in models:\n",
    "    for k in ['error(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_2 = fig_1.add_subplot(212)\n",
    "for model in models:\n",
    "    for k in ['error(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(8.7, 12))\n",
    "ax_3 = fig_2.add_subplot(211)\n",
    "for model in models:\n",
    "    for k in ['acc(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_3.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_4 = fig_2.add_subplot(212)\n",
    "for model in models:\n",
    "    for k in ['acc(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_4.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_ylim(0.0,0.6)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Error(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_ylim(0.0,0.6)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Error(Valid)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "ax_3.legend(models_name,loc=0)\n",
    "ax_3.set_ylim(0.84,1.01)\n",
    "ax_3.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_3.set_ylabel('ACC', fontsize = 12, fontweight = 1000)\n",
    "ax_3.set_title('ACC(Train)', fontsize = 14)\n",
    "ax_4.legend(models_name,loc=0)\n",
    "ax_4.set_ylim(0.84,1.01)\n",
    "ax_4.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_4.set_ylabel('ACC', fontsize = 12, fontweight = 1000)\n",
    "ax_4.set_title('ACC(Valid)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "fig_1.savefig('compare_activation_function_error.pdf')\n",
    "fig_2.savefig('compare_activation_function_acc.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fix learning rate with 5 hidden layers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True):\n",
    "    \n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 10102016 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 50\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = MNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = MNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, LeakyReluLayer, ELULayer, SELULayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit, SELUInit\n",
    "from mlp.learning_rules import GradientDescentLearningRule\n",
    "from mlp.optimisers import Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5 hidden layers LeakyRelu Layer LT = 0.05\n",
    "learning_rate = 0.05  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_5_005 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_5_005[0][-1, LeakyRelu_5_005[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_5_005[0][-1, LeakyRelu_5_005[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_5_005[0][-1, LeakyRelu_5_005[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_5_005[0][-1, LeakyRelu_5_005[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_5_005[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_5_005[0][-1, LeakyRelu_5_005[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_5_005[0][-1, LeakyRelu_5_005[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_5_005[0][-1, LeakyRelu_5_005[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_5_005[0][-1, LeakyRelu_5_005[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5 hidden layers LeakyRelu Layer LT = 0.10\n",
    "learning_rate = 0.10  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_5_010 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_5_010[0][-1, LeakyRelu_5_010[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_5_010[0][-1, LeakyRelu_5_010[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_5_010[0][-1, LeakyRelu_5_010[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_5_010[0][-1, LeakyRelu_5_010[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_5_010[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_5_010[0][-1, LeakyRelu_5_010[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_5_010[0][-1, LeakyRelu_5_010[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_5_010[0][-1, LeakyRelu_5_010[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_5_010[0][-1, LeakyRelu_5_010[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5 hidden layers LeakyRelu Layer LT = 0.20\n",
    "learning_rate = 0.20  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_5_020 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_5_020[0][-1, LeakyRelu_5_020[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_5_020[0][-1, LeakyRelu_5_020[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_5_020[0][-1, LeakyRelu_5_020[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_5_020[0][-1, LeakyRelu_5_020[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_5_020[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_5_020[0][-1, LeakyRelu_5_020[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_5_020[0][-1, LeakyRelu_5_020[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_5_020[0][-1, LeakyRelu_5_020[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_5_020[0][-1, LeakyRelu_5_020[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LeakyRelu_5_005, LeakyRelu_5_010, LeakyRelu_5_020]\n",
    "models_name = ['LeakyRelu_5_005', 'LeakyRelu_5_010', 'LeakyRelu_5_020']\n",
    "fig_1 = plt.figure(figsize=(20, 8))\n",
    "ax_1 = fig_1.add_subplot(121)\n",
    "for model in models:\n",
    "    for k in ['error(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(15.3, 8))\n",
    "ax_2 = fig_2.add_subplot(122)\n",
    "for model in models:\n",
    "    for k in ['error(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Error(Train)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Error(Valid)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "fig_1.savefig('LR-errortrain.pdf')\n",
    "fig_2.savefig('LR-errorvalid.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LeakyRelu_5_005, LeakyRelu_5_010, LeakyRelu_5_020]\n",
    "models_name = ['LeakyRelu_5_005', 'LeakyRelu_5_010', 'LeakyRelu_5_020']\n",
    "fig_1 = plt.figure(figsize=(20, 8))\n",
    "ax_1 = fig_1.add_subplot(121)\n",
    "for model in models:\n",
    "    for k in ['acc(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(15.3, 8))\n",
    "ax_2 = fig_2.add_subplot(122)\n",
    "for model in models:\n",
    "    for k in ['acc(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Acc', fontsize = 12)\n",
    "ax_1.set_title('Acc(Train)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Acc', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Acc(Valid)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "fig_1.savefig('LR-acctrain.pdf')\n",
    "fig_2.savefig('LR-accvalid.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#j = 0\n",
    "#print('| learning_rate | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "#print('|------------|--------------------|--------------------|------------------|------------------|')\n",
    "#for learning_rate in learning_rates:\n",
    "#    print('| {0:.2f}        | {1:.2e}           | {2:.2e}           |  {3:.2f}            | {4:.2f}             |'\n",
    "#          .format(learning_rate, \n",
    "#                  final_errors_train[j], final_errors_valid[j],\n",
    "#                  final_accs_train[j], final_accs_valid[j]))\n",
    "#    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare different number of hidden layers and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers LeakyRelu Layer\n",
    "learning_rate = 0.05  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_2[0][-1, LeakyRelu_2[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_2[0][-1, LeakyRelu_2[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_2[0][-1, LeakyRelu_2[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_2[0][-1, LeakyRelu_2[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_2[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_2[0][-1, LeakyRelu_2[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_2[0][-1, LeakyRelu_2[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_2[0][-1, LeakyRelu_2[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_2[0][-1, LeakyRelu_2[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 hidden layers LeakyRelu Layer\n",
    "learning_rate = 0.05  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_3 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_3[0][-1, LeakyRelu_3[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_3[0][-1, LeakyRelu_3[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_3[0][-1, LeakyRelu_3[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_3[0][-1, LeakyRelu_3[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_3[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_3[0][-1, LeakyRelu_3[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_3[0][-1, LeakyRelu_3[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_3[0][-1, LeakyRelu_3[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_3[0][-1, LeakyRelu_3[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 hidden layers LeakyRelu Layer\n",
    "learning_rate = 0.05  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_4 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_4[0][-1, LeakyRelu_4[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_4[0][-1, LeakyRelu_4[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_4[0][-1, LeakyRelu_4[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_4[0][-1, LeakyRelu_4[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_4[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_4[0][-1, LeakyRelu_4[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_4[0][-1, LeakyRelu_4[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_4[0][-1, LeakyRelu_4[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_4[0][-1, LeakyRelu_4[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5 hidden layers LeakyRelu Layer\n",
    "learning_rate = 0.05  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_5 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_5[0][-1, LeakyRelu_5[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_5[0][-1, LeakyRelu_5[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_5[0][-1, LeakyRelu_5[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_5[0][-1, LeakyRelu_5[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_5[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_5[0][-1, LeakyRelu_5[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_5[0][-1, LeakyRelu_5[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_5[0][-1, LeakyRelu_5[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_5[0][-1, LeakyRelu_5[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6 hidden layers LeakyRelu Layer\n",
    "learning_rate = 0.05 # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_6 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_6[0][-1, LeakyRelu_6[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_6[0][-1, LeakyRelu_6[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_6[0][-1, LeakyRelu_6[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_6[0][-1, LeakyRelu_6[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_6[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_6[0][-1, LeakyRelu_6[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_6[0][-1, LeakyRelu_6[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_6[0][-1, LeakyRelu_6[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_6[0][-1, LeakyRelu_6[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7 hidden layers LeakyRelu Layer\n",
    "learning_rate = 0.05  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_7 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_7[0][-1, LeakyRelu_7[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_7[0][-1, LeakyRelu_7[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_7[0][-1, LeakyRelu_7[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_7[0][-1, LeakyRelu_7[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_7[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_7[0][-1, LeakyRelu_7[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_7[0][-1, LeakyRelu_7[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_7[0][-1, LeakyRelu_7[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_7[0][-1, LeakyRelu_7[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8 hidden layers LeakyRelu Layer\n",
    "learning_rate = 0.05  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "\n",
    "    \n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_8 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_8[0][-1, LeakyRelu_8[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_8[0][-1, LeakyRelu_8[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_8[0][-1, LeakyRelu_8[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_8[0][-1, LeakyRelu_8[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_8[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_8[0][-1, LeakyRelu_8[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_8[0][-1, LeakyRelu_8[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_8[0][-1, LeakyRelu_8[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_8[0][-1, LeakyRelu_8[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LeakyRelu_2, LeakyRelu_3, LeakyRelu_4, \n",
    "          LeakyRelu_5, LeakyRelu_6, LeakyRelu_7, LeakyRelu_8]\n",
    "models_name = ['LeakyRelu_2', 'LeakyRelu_3', 'LeakyRelu_4', \n",
    "               'LeakyRelu_5', 'LeakyRelu_6', 'LeakyRelu_7', 'LeakyRelu_8']\n",
    "fig_1 = plt.figure(figsize=(10, 8))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['error(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(10, 8))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['error(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12)\n",
    "ax_1.set_ylabel('Error', fontsize = 12)\n",
    "ax_1.set_title('Error(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12)\n",
    "ax_2.set_ylabel('Error', fontsize = 12)\n",
    "ax_2.set_title('Error(Valid)', fontsize = 14)\n",
    "fig_1.savefig('numberofhiddenlayer_errt.pdf')\n",
    "fig_2.savefig('numberofhiddenlayer_errv.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LeakyRelu_2, LeakyRelu_3, LeakyRelu_4, \n",
    "          LeakyRelu_5, LeakyRelu_6, LeakyRelu_7, LeakyRelu_8]\n",
    "models_name = ['LeakyRelu_2', 'LeakyRelu_3', 'LeakyRelu_4', \n",
    "               'LeakyRelu_5', 'LeakyRelu_6', 'LeakyRelu_7', 'LeakyRelu_8']\n",
    "fig_1 = plt.figure(figsize=(10, 8))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['acc(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(10, 8))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['acc(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12)\n",
    "ax_1.set_ylabel('Acc', fontsize = 12)\n",
    "ax_1.set_title('Acc(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12)\n",
    "ax_2.set_ylabel('Acc', fontsize = 12)\n",
    "ax_2.set_title('Acc(Valid)', fontsize = 14)\n",
    "fig_1.savefig('numberofhiddenlayer_acct.pdf')\n",
    "fig_2.savefig('numberofhiddenlayer_accv.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FanIn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True):\n",
    "    \n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 10102016 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 50\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = MNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = MNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, LeakyReluLayer, ELULayer, SELULayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit, SELUInit, UniformInit\n",
    "from mlp.learning_rules import GradientDescentLearningRule\n",
    "from mlp.optimisers import Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5 hidden layers LeakyRelu Layer\n",
    "learning_rate = 0.10  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "int_scale_in = np.sqrt(3/input_dim)\n",
    "int_scale_hid = np.sqrt(3/hidden_dim)\n",
    "\n",
    "weights_init_in = UniformInit(-int_scale_in,int_scale_in,rng=rng)\n",
    "weights_init_hid = UniformInit(-int_scale_hid,int_scale_hid,rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init_in, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init_hid, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_5 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_5[0][-1, LeakyRelu_5[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_5[0][-1, LeakyRelu_5[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_5[0][-1, LeakyRelu_5[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_5[0][-1, LeakyRelu_5[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_5[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_5[0][-1, LeakyRelu_5[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_5[0][-1, LeakyRelu_5[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_5[0][-1, LeakyRelu_5[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_5[0][-1, LeakyRelu_5[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6 hidden layers LeakyRelu Layer\n",
    "learning_rate = 0.10 # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "int_scale_in = np.sqrt(3/input_dim)\n",
    "int_scale_hid = np.sqrt(3/hidden_dim)\n",
    "\n",
    "weights_init_in = UniformInit(-int_scale_in,int_scale_in,rng=rng)\n",
    "weights_init_hid = UniformInit(-int_scale_hid,int_scale_hid,rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init_in, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init_hid, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_6 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_6[0][-1, LeakyRelu_6[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_6[0][-1, LeakyRelu_6[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_6[0][-1, LeakyRelu_6[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_6[0][-1, LeakyRelu_6[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_6[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_6[0][-1, LeakyRelu_6[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_6[0][-1, LeakyRelu_6[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_6[0][-1, LeakyRelu_6[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_6[0][-1, LeakyRelu_6[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LeakyRelu_5, LeakyRelu_6]\n",
    "models_name = ['LeakyRelu_5', 'LeakyRelu_6']\n",
    "fig_1 = plt.figure(figsize=(20, 8))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['error(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(15.3, 8))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['error(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Error(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Error(Valid)', fontsize = 14)\n",
    "fig_1.savefig('is_in_errt.pdf')\n",
    "fig_2.savefig('is_in_errv.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LeakyRelu_5, LeakyRelu_6]\n",
    "models_name = ['LeakyRelu_5', 'LeakyRelu_6']\n",
    "fig_1 = plt.figure(figsize=(10, 8))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['acc(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(10, 8))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['acc(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Acc', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Acc(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Acc', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Acc(Valid)', fontsize = 14)\n",
    "fig_1.savefig('is_in_acct.pdf')\n",
    "fig_2.savefig('is_in_accv.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FanOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5 hidden layers LeakyRelu Layer\n",
    "learning_rate = 0.10  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "int_scale_hid = np.sqrt(3/hidden_dim)\n",
    "int_scale_out = np.sqrt(3/output_dim)\n",
    "\n",
    "weights_init_hid = UniformInit(-int_scale_hid,int_scale_hid,rng=rng)\n",
    "weights_init_out = UniformInit(-int_scale_out,int_scale_out,rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init_out, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_5 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_5[0][-1, LeakyRelu_5[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_5[0][-1, LeakyRelu_5[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_5[0][-1, LeakyRelu_5[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_5[0][-1, LeakyRelu_5[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_5[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_5[0][-1, LeakyRelu_5[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_5[0][-1, LeakyRelu_5[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_5[0][-1, LeakyRelu_5[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_5[0][-1, LeakyRelu_5[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6 hidden layers LeakyRelu Layer\n",
    "learning_rate = 0.10 # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "int_scale_hid = np.sqrt(3/hidden_dim)\n",
    "int_scale_out = np.sqrt(3/output_dim)\n",
    "\n",
    "weights_init_hid = UniformInit(-int_scale_hid,int_scale_hid,rng=rng)\n",
    "weights_init_out = UniformInit(-int_scale_out,int_scale_out,rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init_out, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_6 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_6[0][-1, LeakyRelu_6[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_6[0][-1, LeakyRelu_6[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_6[0][-1, LeakyRelu_6[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_6[0][-1, LeakyRelu_6[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_6[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_6[0][-1, LeakyRelu_6[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_6[0][-1, LeakyRelu_6[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_6[0][-1, LeakyRelu_6[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_6[0][-1, LeakyRelu_6[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LeakyRelu_5, LeakyRelu_6]\n",
    "models_name = ['LeakyRelu_5', 'LeakyRelu_6']\n",
    "fig_1 = plt.figure(figsize=(10, 8))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['error(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(10, 8))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['error(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Error(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Error(Valid)', fontsize = 14)\n",
    "fig_1.savefig('is_out_errt.pdf')\n",
    "fig_2.savefig('is_out_errv.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LeakyRelu_5, LeakyRelu_6]\n",
    "models_name = ['LeakyRelu_5', 'LeakyRelu_6']\n",
    "fig_1 = plt.figure(figsize=(10, 8))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['acc(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(10, 8))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['acc(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Acc', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Acc(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Acc', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Acc(Valid)', fontsize = 14)\n",
    "fig_1.savefig('is_out_acct.pdf')\n",
    "fig_2.savefig('is_out_accv.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FanInOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 10102016 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 50\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = MNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = MNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, LeakyReluLayer, ELULayer, SELULayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit, SELUInit\n",
    "from mlp.learning_rules import GradientDescentLearningRule\n",
    "from mlp.optimisers import Optimiser\n",
    "from mlp.initialisers import UniformInit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 hidden layers LeakyRelu Layer\n",
    "learning_rate = 0.10  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "int_scale_in = np.sqrt(6/input_dim+hidden_dim)\n",
    "int_scale_hid = np.sqrt(6/hidden_dim+hidden_dim)\n",
    "int_scale_out = np.sqrt(6/hidden_dim+output_dim)\n",
    "\n",
    "weights_init_in = UniformInit(-int_scale_in,int_scale_in,rng=rng)\n",
    "weights_init_hid = UniformInit(-int_scale_hid,int_scale_hid,rng=rng)\n",
    "weights_init_out = UniformInit(-int_scale_out,int_scale_out,rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init_in, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init_out, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_4 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_4[0][-1, LeakyRelu_4[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_4[0][-1, LeakyRelu_4[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_4[0][-1, LeakyRelu_4[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_4[0][-1, LeakyRelu_4[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_4[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_4[0][-1, LeakyRelu_4[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_4[0][-1, LeakyRelu_4[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_4[0][-1, LeakyRelu_4[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_4[0][-1, LeakyRelu_4[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers LeakyRelu Layer\n",
    "learning_rate = 0.10 # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "int_scale_in = np.sqrt(6/input_dim+hidden_dim)\n",
    "int_scale_hid = np.sqrt(6/hidden_dim+hidden_dim)\n",
    "int_scale_out = np.sqrt(6/hidden_dim+output_dim)\n",
    "\n",
    "weights_init_in = UniformInit(-int_scale_in,int_scale_in,rng=rng)\n",
    "weights_init_hid = UniformInit(-int_scale_hid,int_scale_hid,rng=rng)\n",
    "weights_init_out = UniformInit(-int_scale_out,int_scale_out,rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init_in, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init_out, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "LeakyRelu_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(LeakyRelu_2[0][-1, LeakyRelu_2[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(LeakyRelu_2[0][-1, LeakyRelu_2[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(LeakyRelu_2[0][-1, LeakyRelu_2[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(LeakyRelu_2[0][-1, LeakyRelu_2[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(LeakyRelu_2[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(LeakyRelu_2[0][-1, LeakyRelu_2[1]['error(train)']])\n",
    "final_errors_valid.append(LeakyRelu_2[0][-1, LeakyRelu_2[1]['error(valid)']])\n",
    "final_accs_train.append(LeakyRelu_2[0][-1, LeakyRelu_2[1]['acc(train)']])\n",
    "final_accs_valid.append(LeakyRelu_2[0][-1, LeakyRelu_2[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LeakyRelu_2]\n",
    "models_name = ['LeakyRelu_2']\n",
    "fig_1 = plt.figure(figsize=(10, 8))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['error(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(10, 8))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['error(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Error(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Error(Valid)', fontsize = 14)\n",
    "fig_1.savefig('is_inout_errt.pdf')\n",
    "fig_2.savefig('is_inout_errv.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LeakyRelu_2]\n",
    "models_name = ['LeakyRelu_2']\n",
    "fig_1 = plt.figure(figsize=(10, 8))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['acc(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(10, 8))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['acc(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Acc', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Acc(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Acc', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Acc(Valid)', fontsize = 14)\n",
    "fig_1.savefig('is_inout_acct.pdf')\n",
    "fig_2.savefig('is_inout_accv.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FanInNor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True):\n",
    "    \n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 10102016 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 50\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = MNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = MNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, LeakyReluLayer, ELULayer, SELULayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit, SELUInit\n",
    "from mlp.learning_rules import GradientDescentLearningRule\n",
    "from mlp.optimisers import Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers SELU Layer\n",
    "learning_rate = 0.1\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init_in = SELUInit(0,input_dim,rng=rng)\n",
    "weights_init_hid = SELUInit(0,hidden_dim,rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init_in, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init_hid, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "SELULayer_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "print('    final error(train) = {0:.2e}'.format(SELULayer_2[0][-1, SELULayer_2[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(SELULayer_2[0][-1, SELULayer_2[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(SELULayer_2[0][-1, SELULayer_2[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(SELULayer_2[0][-1, SELULayer_2[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(SELULayer_2[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(SELULayer_2[0][-1, SELULayer_2[1]['error(train)']])\n",
    "final_errors_valid.append(SELULayer_2[0][-1, SELULayer_2[1]['error(valid)']])\n",
    "final_accs_train.append(SELULayer_2[0][-1, SELULayer_2[1]['acc(train)']])\n",
    "final_accs_valid.append(SELULayer_2[0][-1, SELULayer_2[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 hidden layers SELULayer Layer\n",
    "learning_rate = 0.10  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "\n",
    "weights_init_in = SELUInit(0,input_dim,rng=rng)\n",
    "weights_init_hid = SELUInit(0,hidden_dim,rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init_in, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init_hid, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "SELULayer_3 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(SELULayer_3[0][-1, SELULayer_3[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(SELULayer_3[0][-1, SELULayer_3[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(SELULayer_3[0][-1, SELULayer_3[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(SELULayer_3[0][-1, SELULayer_3[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(SELULayer_3[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(SELULayer_3[0][-1, SELULayer_3[1]['error(train)']])\n",
    "final_errors_valid.append(SELULayer_3[0][-1, SELULayer_3[1]['error(valid)']])\n",
    "final_accs_train.append(SELULayer_3[0][-1, SELULayer_3[1]['acc(train)']])\n",
    "final_accs_valid.append(SELULayer_3[0][-1, SELULayer_3[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [SELULayer_2, SELULayer_3]\n",
    "models_name = ['SELULayer_2', 'SELULayer_3']\n",
    "fig_1 = plt.figure(figsize=(20, 8))\n",
    "ax_1 = fig_1.add_subplot(121)\n",
    "for model in models:\n",
    "    for k in ['error(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(15.3, 8))\n",
    "ax_2 = fig_2.add_subplot(122)\n",
    "for model in models:\n",
    "    for k in ['error(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Error(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Error(Valid)', fontsize = 14)\n",
    "fig_1.savefig('noris_in_errt.pdf')\n",
    "fig_2.savefig('noris_in_errv.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [SELULayer_2, SELULayer_3]\n",
    "models_name = ['SELULayer_2', 'SELULayer_3']\n",
    "fig_1 = plt.figure(figsize=(10, 8))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['acc(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(10, 8))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['acc(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Acc', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Acc(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Acc', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Acc(Valid)', fontsize = 14)\n",
    "fig_1.savefig('noris_in_acct.pdf')\n",
    "fig_2.savefig('noris_in_accv.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FanOutNor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True):\n",
    "    \n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 10102016 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 50\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = MNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = MNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, LeakyReluLayer, ELULayer, SELULayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit, SELUInit, NormalInit\n",
    "from mlp.learning_rules import GradientDescentLearningRule\n",
    "from mlp.optimisers import Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers SELU Layer\n",
    "learning_rate = 0.1\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init_hid = SELUInit(0,hidden_dim,rng=rng)\n",
    "weights_init_out = SELUInit(0,output_dim,rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init_out, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "SELULayer_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "print('    final error(train) = {0:.2e}'.format(SELULayer_2[0][-1, SELULayer_2[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(SELULayer_2[0][-1, SELULayer_2[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(SELULayer_2[0][-1, SELULayer_2[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(SELULayer_2[0][-1, SELULayer_2[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(SELULayer_2[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(SELULayer_2[0][-1, SELULayer_2[1]['error(train)']])\n",
    "final_errors_valid.append(SELULayer_2[0][-1, SELULayer_2[1]['error(valid)']])\n",
    "final_accs_train.append(SELULayer_2[0][-1, SELULayer_2[1]['acc(train)']])\n",
    "final_accs_valid.append(SELULayer_2[0][-1, SELULayer_2[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 hidden layers SELULayer Layer\n",
    "learning_rate = 0.10  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "\n",
    "weights_init_hid = SELUInit(0,hidden_dim,rng=rng)\n",
    "weights_init_out = SELUInit(0,output_dim,rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init_out, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "SELULayer_3 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(SELULayer_3[0][-1, SELULayer_3[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(SELULayer_3[0][-1, SELULayer_3[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(SELULayer_3[0][-1, SELULayer_3[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(SELULayer_3[0][-1, SELULayer_3[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(SELULayer_3[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(SELULayer_3[0][-1, SELULayer_3[1]['error(train)']])\n",
    "final_errors_valid.append(SELULayer_3[0][-1, SELULayer_3[1]['error(valid)']])\n",
    "final_accs_train.append(SELULayer_3[0][-1, SELULayer_3[1]['acc(train)']])\n",
    "final_accs_valid.append(SELULayer_3[0][-1, SELULayer_3[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [SELULayer_2, SELULayer_3]\n",
    "models_name = ['SELULayer_2', 'SELULayer_3']\n",
    "fig_1 = plt.figure(figsize=(10, 8))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['error(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(10, 8))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['error(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Error(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Error(Valid)', fontsize = 14)\n",
    "fig_1.savefig('noris_out_errt.pdf')\n",
    "fig_2.savefig('noris_out_errv.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [SELULayer_2, SELULayer_3]\n",
    "models_name = ['SELULayer_2', 'SELULayer_3']\n",
    "fig_1 = plt.figure(figsize=(10, 8))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['acc(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(10, 8))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['acc(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Acc', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Acc(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Acc', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Acc(Valid)', fontsize = 14)\n",
    "fig_1.savefig('noris_out_acct.pdf')\n",
    "fig_2.savefig('noris_out_accv.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FanInOutNor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True):\n",
    "    \n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 10102016 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 50\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = MNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = MNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 10102016 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 50\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = MNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = MNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers SELU Layer\n",
    "learning_rate = 0.1\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init_in = SELUInit(0,input_dim,rng=rng)\n",
    "weights_init_hid = SELUInit(0,hidden_dim,rng=rng)\n",
    "weights_init_out = SELUInit(0,output_dim,rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init_in, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init_out, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "SELULayer_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "print('    final error(train) = {0:.2e}'.format(SELULayer_2[0][-1, SELULayer_2[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(SELULayer_2[0][-1, SELULayer_2[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(SELULayer_2[0][-1, SELULayer_2[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(SELULayer_2[0][-1, SELULayer_2[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(SELULayer_2[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(SELULayer_2[0][-1, SELULayer_2[1]['error(train)']])\n",
    "final_errors_valid.append(SELULayer_2[0][-1, SELULayer_2[1]['error(valid)']])\n",
    "final_accs_train.append(SELULayer_2[0][-1, SELULayer_2[1]['acc(train)']])\n",
    "final_accs_valid.append(SELULayer_2[0][-1, SELULayer_2[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 hidden layers SELULayer Layer\n",
    "learning_rate = 0.10  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init_in = SELUInit(0,input_dim,rng=rng)\n",
    "weights_init_hid = SELUInit(0,hidden_dim,rng=rng)\n",
    "weights_init_out = SELUInit(0,output_dim,rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init_in, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init_hid, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init_out, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "SELULayer_3 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(SELULayer_3[0][-1, SELULayer_3[1]['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(SELULayer_3[0][-1, SELULayer_3[1]['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(SELULayer_3[0][-1, SELULayer_3[1]['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(SELULayer_3[0][-1, SELULayer_3[1]['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(SELULayer_3[2] * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(SELULayer_3[0][-1, SELULayer_3[1]['error(train)']])\n",
    "final_errors_valid.append(SELULayer_3[0][-1, SELULayer_3[1]['error(valid)']])\n",
    "final_accs_train.append(SELULayer_3[0][-1, SELULayer_3[1]['acc(train)']])\n",
    "final_accs_valid.append(SELULayer_3[0][-1, SELULayer_3[1]['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [SELULayer_2, SELULayer_3]\n",
    "models_name = ['SELULayer_2', 'SELULayer_3']\n",
    "fig_1 = plt.figure(figsize=(10, 8))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['error(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(10, 8))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['error(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Error(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Error(Valid)', fontsize = 14)\n",
    "fig_1.savefig('noris_inout_errt.pdf')\n",
    "fig_2.savefig('noris_inout_errv.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [SELULayer_2, SELULayer_3]\n",
    "models_name = ['SELULayer_2', 'SELULayer_3']\n",
    "fig_1 = plt.figure(figsize=(10, 8))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['acc(train)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "fig_2 = plt.figure(figsize=(10, 8))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for model in models:\n",
    "    for k in ['acc(valid)']:\n",
    "        stats = model[0]\n",
    "        keys = model[1]\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "ax_1.legend(models_name,loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Acc', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Acc(Train)', fontsize = 14)\n",
    "ax_2.legend(models_name,loc=0)\n",
    "ax_2.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_ylabel('Acc', fontsize = 12, fontweight = 1000)\n",
    "ax_2.set_title('Acc(Valid)', fontsize = 14)\n",
    "fig_1.savefig('noris_inout_acct.pdf')\n",
    "fig_2.savefig('noris_inout_accv.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
