{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s17/s1773005/miniconda3/envs/mlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "inputs = tf.placeholder(tf.float32, [None,120,25,1], 'inputs')\n",
    "targets = tf.placeholder(tf.float32, [None, 10], 'targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_and_accuracy(data):\n",
    "    \"\"\"Calculate average error and classification accuracy across a dataset.\n",
    "    \n",
    "    Args:\n",
    "        data: Data provider which iterates over input-target batches in dataset.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple with first element scalar value corresponding to average error\n",
    "        across all batches in dataset and second value corresponding to\n",
    "        average classification accuracy across all batches in dataset.\n",
    "    \"\"\"\n",
    "    err = 0\n",
    "    acc = 0\n",
    "    for input_batch, target_batch in data:\n",
    "        err += sess.run(error, feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        acc += sess.run(accuracy, feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    err /= data.num_batches\n",
    "    acc /= data.num_batches\n",
    "    return err, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, w):\n",
    "    return tf.nn.conv2d(x, w, strides = [1, 1, 1, 1], padding = 'VALID')\n",
    "def pool(x, m, n):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        x: outputs of the last layer\n",
    "        m: Raw of the max_pooling\n",
    "        n: Column of the max_pooling\n",
    "    \n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(x, ksize = [1, m, n, 1], strides = [1, m, n, 1], padding = 'VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1: running error average = 3.94 running acc average = 0.11\n",
      "End of epoch 2: running error average = 2.31 running acc average = 0.10\n",
      "End of epoch 3: running error average = 2.31 running acc average = 0.10\n",
      "End of epoch 4: running error average = 2.31 running acc average = 0.10\n",
      "End of epoch 5: running error average = 2.31 running acc average = 0.10\n",
      "End of epoch 6: running error average = 2.31 running acc average = 0.10\n",
      "End of epoch 7: running error average = 2.31 running acc average = 0.10\n",
      "End of epoch 8: running error average = 2.31 running acc average = 0.10\n",
      "End of epoch 9: running error average = 2.31 running acc average = 0.10\n",
      "End of epoch 10: running error average = 2.31 running acc average = 0.10\n",
      "End of epoch 11: running error average = 2.31 running acc average = 0.10\n"
     ]
    }
   ],
   "source": [
    "weights = {\n",
    "    'c1': tf.Variable(tf.random_normal([5, 5, 1, 5])),\n",
    "    'c2': tf.Variable(tf.random_normal([5, 5, 5, 10])),\n",
    "    'out': tf.Variable(tf.random_normal([5*1*10, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([5])),\n",
    "    'b2': tf.Variable(tf.random_normal([10])),\n",
    "    'out': tf.Variable(tf.random_normal([10]))\n",
    "}\n",
    "\n",
    "def multilayer_cnn(data):\n",
    "    conv1 = tf.nn.tanh(conv2d(data, weights['c1']) + biases['b1'])\n",
    "    pool1 = pool(conv1, 4, 3)\n",
    "    conv2 = tf.nn.tanh(conv2d(pool1, weights['c2']) + biases['b2'])\n",
    "    pool2 = pool(conv2, 5, 3)\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 5*1*10])\n",
    "    out_layer = tf.matmul(pool2_flat, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "out_layer = multilayer_cnn(inputs)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "#regularizer = tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(weights['h2'])+tf.nn.l2_loss(weights['out'])\n",
    "#error = tf.reduce_mean(error + beta * regularizer)\n",
    "\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(out_layer, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(error)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "import data_providers_cnn as data_providers_cnn\n",
    "train_data = data_providers_cnn.MSD10GenreDataProvider('train', batch_size=50, flatten=True, one_hot=True)\n",
    "valid_data = data_providers_cnn.MSD10GenreDataProvider('valid', batch_size=50, flatten=True, one_hot=True)\n",
    "\n",
    "num_epoch = 50\n",
    "error_train = []\n",
    "error_valid = []\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    running_error = 0.\n",
    "    running_acc = 0.\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [train_step, error, accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_acc += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_acc /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e + 1, running_error), \n",
    "          'running acc average = {1:.2f}'.format(e + 1, running_acc))\n",
    "    error_train = np.append(error_train, running_error)\n",
    "    acc_train = np.append(acc_train, running_acc)\n",
    "    a = get_error_and_accuracy(valid_data)\n",
    "    error_valid = np.append(error_valid, a[0])\n",
    "    acc_valid = np.append(acc_valid, a[1])\n",
    "    \n",
    "CNN_2layers = [error_train, acc_train, error_valid, acc_valid]\n",
    "np.save('CNN_2layers', CNN_2layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('CNN_2layers.npy')\n",
    "\n",
    "fig_1 = plt.figure(figsize=(10,6))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "ax_1.plot(np.arange(data[0].shape[0]), data[0], label = 'train')\n",
    "ax_1.plot(np.arange(data[2].shape[0]), data[2], label = 'valid')\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('CNN_2layers(error)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "fig_1.savefig('CNN_2layers(error).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('CNN_2layers.npy')\n",
    "\n",
    "fig_1 = plt.figure(figsize=(10,6))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "ax_1.plot(np.arange(data[1].shape[0]), data[1], label = 'train')\n",
    "ax_1.plot(np.arange(data[3].shape[0]), data[3], label = 'valid')\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('ACC', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('CNN_2layers(acc)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "fig_1.savefig('CNN_2layers(acc).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'c1': tf.Variable(tf.random_normal([5, 1, 1, 5])),\n",
    "    'c2': tf.Variable(tf.random_normal([5, 1, 5, 10])),\n",
    "    'c3': tf.Variable(tf.random_normal([5, 1, 10, 10])),\n",
    "    'out': tf.Variable(tf.random_normal([2*25*10,10]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([5])),\n",
    "    'b2': tf.Variable(tf.random_normal([10])),\n",
    "    'b3': tf.Variable(tf.random_normal([10])),\n",
    "    'out': tf.Variable(tf.random_normal([10]))\n",
    "}\n",
    "\n",
    "def multilayer_cnn(data):\n",
    "    conv1 = tf.nn.tanh(conv2d(data, weights['c1']) + biases['b1'])\n",
    "    pool1 = pool(conv1, 2, 1)\n",
    "    conv2 = tf.nn.tanh(conv2d(pool1, weights['c2']) + biases['b2'])\n",
    "    pool2 = pool(conv2, 3, 1)\n",
    "    conv3 = tf.nn.tanh(conv2d(pool2, weights['c3']) + biases['b3'])\n",
    "    pool3 = pool(conv3, 7, 1)\n",
    "    pool3_flat = tf.reshape(pool3, [-1, 2*25*10])\n",
    "    out_layer = tf.matmul(pool3_flat, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "out_layer = multilayer_cnn(inputs)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "#regularizer = tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(weights['h2'])+tf.nn.l2_loss(weights['out'])\n",
    "#error = tf.reduce_mean(error + beta * regularizer)\n",
    "\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(out_layer, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(error)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "import data_providers as data_providers\n",
    "train_data = data_providers.MSD10GenreDataProvider('train', batch_size=50, flatten=True, one_hot=True)\n",
    "valid_data = data_providers.MSD10GenreDataProvider('valid', batch_size=50, flatten=True, one_hot=True)\n",
    "\n",
    "num_epoch = 50\n",
    "error_train = []\n",
    "error_valid = []\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    running_error = 0.\n",
    "    running_acc = 0.\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [train_step, error, accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_acc += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_acc /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e + 1, running_error), \n",
    "          'running acc average = {1:.2f}'.format(e + 1, running_acc))\n",
    "    error_train = np.append(error_train, running_error)\n",
    "    acc_train = np.append(acc_train, running_acc)\n",
    "    a = get_error_and_accuracy(valid_data)\n",
    "    error_valid = np.append(error_valid, a[0])\n",
    "    acc_valid = np.append(acc_valid, a[1])\n",
    "    \n",
    "CNN_3layers = [error_train, acc_train, error_valid, acc_valid]\n",
    "np.save('CNN_3layers', CNN_3layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('CNN_3layers.npy')\n",
    "\n",
    "fig_1 = plt.figure(figsize=(10,6))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "ax_1.plot(np.arange(data[0].shape[0]), data[0], label = 'train')\n",
    "ax_1.plot(np.arange(data[2].shape[0]), data[2], label = 'valid')\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('CNN_3layers(error)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "fig_1.savefig('CNN_3layers(error).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('CNN_3layers.npy')\n",
    "\n",
    "fig_1 = plt.figure(figsize=(10,6))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "ax_1.plot(np.arange(data[1].shape[0]), data[1], label = 'train')\n",
    "ax_1.plot(np.arange(data[3].shape[0]), data[3], label = 'valid')\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('ACC', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('CNN_3layers(acc)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "fig_1.savefig('CNN_3layers(acc).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'c1': tf.Variable(tf.random_normal([5, 1, 1, 5])),\n",
    "    'c2': tf.Variable(tf.random_normal([5, 1, 5, 10])),\n",
    "    'c3': tf.Variable(tf.random_normal([5, 1, 10, 10])),\n",
    "    'd1': tf.Variable(tf.random_normal([7*25*10,350])),\n",
    "    'd2': tf.Variable(tf.random_normal([350,100])),\n",
    "    'out': tf.Variable(tf.random_normal([100,10]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([5])),\n",
    "    'b2': tf.Variable(tf.random_normal([10])),\n",
    "    'b3': tf.Variable(tf.random_normal([10])),\n",
    "    'd1': tf.Variable(tf.random_normal([350])),\n",
    "    'd2': tf.Variable(tf.random_normal([100])),\n",
    "    'out': tf.Variable(tf.random_normal([10]))\n",
    "}\n",
    "\n",
    "def multilayer_cnn(data):\n",
    "    conv1 = tf.nn.tanh(conv2d(data, weights['c1']) + biases['b1'])\n",
    "    pool1 = pool(conv1, 2, 1)\n",
    "    conv2 = tf.nn.tanh(conv2d(pool1, weights['c2']) + biases['b2'])\n",
    "    pool2 = pool(conv2, 3, 1)\n",
    "    conv3 = tf.nn.tanh(conv2d(pool2, weights['c3']) + biases['b3'])\n",
    "    pool3 = pool(conv3, 2, 1)\n",
    "    pool3_flat = tf.reshape(pool3, [-1, 7*25*10])\n",
    "    d1_layer = tf.nn.tanh(tf.matmul(pool3_flat, weights['d1']) + biases['d1'])\n",
    "    d2_layer = tf.nn.tanh(tf.matmul(d1_layer, weights['d2']) + biases['d2'])\n",
    "    out_layer = tf.matmul(d2_layer, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "out_layer = multilayer_cnn(inputs)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "#regularizer = tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(weights['h2'])+tf.nn.l2_loss(weights['out'])\n",
    "#error = tf.reduce_mean(error + beta * regularizer)\n",
    "\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(out_layer, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(error)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "import data_providers as data_providers\n",
    "train_data = data_providers.MSD10GenreDataProvider('train', batch_size=50, flatten=True, one_hot=True)\n",
    "valid_data = data_providers.MSD10GenreDataProvider('valid', batch_size=50, flatten=True, one_hot=True)\n",
    "\n",
    "num_epoch = 50\n",
    "error_train = []\n",
    "error_valid = []\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    running_error = 0.\n",
    "    running_acc = 0.\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [train_step, error, accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_acc += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_acc /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e + 1, running_error), \n",
    "          'running acc average = {1:.2f}'.format(e + 1, running_acc))\n",
    "    error_train = np.append(error_train, running_error)\n",
    "    acc_train = np.append(acc_train, running_acc)\n",
    "    a = get_error_and_accuracy(valid_data)\n",
    "    error_valid = np.append(error_valid, a[0])\n",
    "    acc_valid = np.append(acc_valid, a[1])\n",
    "    \n",
    "CNN_3cnn_2d = [error_train, acc_train, error_valid, acc_valid]\n",
    "np.save('CNN_3cnn_2d', CNN_3cnn_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('CNN_3cnn_2d.npy')\n",
    "\n",
    "fig_1 = plt.figure(figsize=(10,6))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "ax_1.plot(np.arange(data[0].shape[0]), data[0], label = 'train')\n",
    "ax_1.plot(np.arange(data[2].shape[0]), data[2], label = 'valid')\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('CNN_3cnn_2d(error)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "fig_1.savefig('CNN_3cnn_2d(error).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CNN_3cnn_2d\n",
    "fig_1 = plt.figure(figsize=(10,6))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "ax_1.plot(np.arange(data[1].shape[0]), data[1], label = 'train')\n",
    "ax_1.plot(np.arange(data[3].shape[0]), data[3], label = 'valid')\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('ACC', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('CNN_3cnn_2d(acc)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "fig_1.savefig('CNN_3cnn_2d(acc).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
