{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s17/s1773005/miniconda3/envs/mlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "inputs = tf.placeholder(tf.float32, [None, 3000], 'inputs')\n",
    "targets = tf.placeholder(tf.float32, [None, 10], 'targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_and_accuracy(data):\n",
    "    \"\"\"Calculate average error and classification accuracy across a dataset.\n",
    "    \n",
    "    Args:\n",
    "        data: Data provider which iterates over input-target batches in dataset.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple with first element scalar value corresponding to average error\n",
    "        across all batches in dataset and second value corresponding to\n",
    "        average classification accuracy across all batches in dataset.\n",
    "    \"\"\"\n",
    "    err = 0\n",
    "    acc = 0\n",
    "    for input_batch, target_batch in data:\n",
    "        err += sess.run(error, feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        acc += sess.run(accuracy, feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    err /= data.num_batches\n",
    "    acc /= data.num_batches\n",
    "    return err, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1: running error average = 6324.06 running acc average = 0.25\n",
      "End of epoch 2: running error average = 4718.55 running acc average = 0.33\n",
      "End of epoch 3: running error average = 4131.62 running acc average = 0.39\n"
     ]
    }
   ],
   "source": [
    "n_hidden_1 = 200\n",
    "n_hidden_2 = 200\n",
    "beta = 0.01 #0.009\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([3000, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([10]))\n",
    "}\n",
    "def multilayer_3hidden100(data):\n",
    "    layer_1 = tf.nn.relu(tf.matmul(data, weights['h1']) + biases['b1'])\n",
    "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "out_layer = multilayer_3hidden100(inputs)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "regularizer = tf.nn.l2_loss(weights['h1'])+tf.nn.l2_loss(weights['h2'])+tf.nn.l2_loss(weights['out'])\n",
    "error = tf.reduce_mean(error + beta * regularizer)\n",
    "\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(out_layer, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(error)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "import data_providers as data_providers\n",
    "train_data = data_providers.MSD10GenreDataProvider('train', batch_size=50, flatten=True, one_hot=True)\n",
    "valid_data = data_providers.MSD10GenreDataProvider('valid', batch_size=50, flatten=True, one_hot=True)\n",
    "\n",
    "num_epoch = 100\n",
    "error_train = []\n",
    "error_valid = []\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    running_error = 0.\n",
    "    running_acc = 0.\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [train_step, error, accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_acc += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_acc /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e + 1, running_error), \n",
    "          'running acc average = {1:.2f}'.format(e + 1, running_acc))\n",
    "    error_train = np.append(error_train, running_error)\n",
    "    acc_train = np.append(acc_train, running_acc)\n",
    "    a = get_error_and_accuracy(valid_data)\n",
    "    error_valid = np.append(error_valid, a[0])\n",
    "    acc_valid = np.append(acc_valid, a[1])\n",
    "    \n",
    "Adam_3layers_200_L2_100 = [error_train, acc_train, error_valid, acc_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Adam_3layers_200_L2_100\n",
    "\n",
    "fig_1 = plt.figure(figsize=(10,6))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "ax_1.plot(np.arange(data[0].shape[0]), data[0], label = 'train')\n",
    "ax_1.plot(np.arange(data[2].shape[0]), data[2], label = 'valid')\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Adam_3layers_200_L2(error)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "fig_1.savefig('Adam_3layers_200_L2(error)_100.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Adam_3layers_200_L2_100\n",
    "\n",
    "fig_1 = plt.figure(figsize=(10,6))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "ax_1.plot(np.arange(data[1].shape[0]), data[1], label = 'train')\n",
    "ax_1.plot(np.arange(data[3].shape[0]), data[3], label = 'valid')\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('ACC', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Adam_3layers_200_L2(acc)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "fig_1.savefig('Adam_3layers_200_L2(acc)_100.pdf')\n",
    "print(data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 300\n",
    "beta = 0.01 #0.009\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([3000, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([10]))\n",
    "}\n",
    "def multilayer_3hidden100(data):\n",
    "    layer_1 = tf.nn.relu(tf.matmul(data, weights['h1']) + biases['b1'])\n",
    "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "out_layer = multilayer_3hidden100(inputs)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "regularizer = tf.nn.l2_loss(weights['h1'])+tf.nn.l2_loss(weights['h2'])+tf.nn.l2_loss(weights['out'])\n",
    "error = tf.reduce_mean(error + beta * regularizer)\n",
    "\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(out_layer, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(error)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "import data_providers as data_providers\n",
    "train_data = data_providers.MSD10GenreDataProvider('train', batch_size=50, flatten=True, one_hot=True)\n",
    "valid_data = data_providers.MSD10GenreDataProvider('valid', batch_size=50, flatten=True, one_hot=True)\n",
    "\n",
    "num_epoch = 100\n",
    "error_train = []\n",
    "error_valid = []\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    running_error = 0.\n",
    "    running_acc = 0.\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [train_step, error, accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_acc += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_acc /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e + 1, running_error), \n",
    "          'running acc average = {1:.2f}'.format(e + 1, running_acc))\n",
    "    error_train = np.append(error_train, running_error)\n",
    "    acc_train = np.append(acc_train, running_acc)\n",
    "    a = get_error_and_accuracy(valid_data)\n",
    "    error_valid = np.append(error_valid, a[0])\n",
    "    acc_valid = np.append(acc_valid, a[1])\n",
    "    \n",
    "Adam_3layers_300_L2_100 = [error_train, acc_train, error_valid, acc_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Adam_3layers_300_L2_100\n",
    "\n",
    "fig_1 = plt.figure(figsize=(10,6))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "ax_1.plot(np.arange(data[0].shape[0]), data[0], label = 'train')\n",
    "ax_1.plot(np.arange(data[2].shape[0]), data[2], label = 'valid')\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('Error', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Adam_3layers_300_L2(error)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "fig_1.savefig('Adam_3layers_300_L2(error)_100.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Adam_3layers_300_L2_100\n",
    "\n",
    "fig_1 = plt.figure(figsize=(10,6))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "ax_1.plot(np.arange(data[1].shape[0]), data[1], label = 'train')\n",
    "ax_1.plot(np.arange(data[3].shape[0]), data[3], label = 'valid')\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number',fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_ylabel('ACC', fontsize = 12, fontweight = 1000)\n",
    "ax_1.set_title('Adam_3layers_300_L2(acc)', fontsize = 14)\n",
    "plt.tight_layout()\n",
    "fig_1.savefig('Adam_3layers_300_L2(acc)_100.pdf')\n",
    "print(data[1])\n",
    "print(data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Adam_3layers_200_L2_100', Adam_3layers_200_L2_100)\n",
    "np.save('Adam_3layers_300_L2_100', Adam_3layers_300_L2_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
