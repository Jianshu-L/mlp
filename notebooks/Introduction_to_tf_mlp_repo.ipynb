{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Experimentation Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the previous tutorial we introduced the tensorflow framework and some very basic functionality that it can provide. In this tutorial we will present a far more readable and research oriented tensorflow based code-base that allows one to quickly build new model architectures and research experiments in tensorflow. The proposed code-structure has been tested in real research and has proven a very readable and easily modifiable setup. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf_mlp package contains all necessary modules with which one can easily train and evaluate a classifier. The following packages can be found:\n",
    "\n",
    "    1. utils: \n",
    "        1. network_summary: Provides utilities with which one can get network summaries, such as the number of parameters and names of layers.\n",
    "        2. parser_utils which are used to parse arguments passed to the training scripts.\n",
    "      3. storage, which is responsible for storing network statistics.\n",
    "    2. data_providers.py : Provides the data providers for training, validation and testing.\n",
    "    3. network_architectures.py: Defines the network architectures. We provide VGGNet as an example.\n",
    "    4. network_builder.py: Builds the tensorflow computation graph. In more detail, it builds the losses, tensorflow summaries and training operations.\n",
    "    5. network_trainer.py: Runs an experiment, composed of training, validation and testing. It is setup to use arguments such that one can easily write multiple bash scripts with different hyperparameters and run experiments very quickly with minimal code changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run an experiment just run:\n",
    "\n",
    "```\n",
    "python network_trainer.py --batch_size 128 --epochs 100 --experiment_prefix VGG_EMNIST --tensorboard_use True --batch_norm_use True --strided_dim_reduction True --seed 16122017\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments after network_trainer.py can be changed to suit your experimental needs. For more arguments and exploring how to add new arguments of your own please view parser_utils.py under utils and network_trainer.py as they provide all the functionality that is necessary to add arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally remember to make sure your code is not just efficient but readable. Research code has a very bad reputation, so let's try to improve readability one research line code at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run tensorboard just point tensorboard to the correct logs repository as follows:\n",
    "    ```tensorboard --port 60xx --logdir /path/to/logs```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s17/s1773005/miniconda3/envs/mlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "inputs = tf.placeholder(tf.float32, [None, 3000], 'inputs')\n",
    "targets = tf.placeholder(tf.float32, [None, 10], 'targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1: running error average = 509.86 running acc average = 0.20\n",
      "End of epoch 2: running error average = 54.51 running acc average = 0.18\n",
      "End of epoch 3: running error average = 21.51 running acc average = 0.17\n",
      "End of epoch 4: running error average = 12.92 running acc average = 0.16\n",
      "End of epoch 5: running error average = 9.64 running acc average = 0.15\n",
      "End of epoch 6: running error average = 7.53 running acc average = 0.16\n",
      "End of epoch 7: running error average = 6.30 running acc average = 0.15\n",
      "End of epoch 8: running error average = 5.44 running acc average = 0.15\n",
      "End of epoch 9: running error average = 5.00 running acc average = 0.15\n",
      "End of epoch 10: running error average = 4.48 running acc average = 0.15\n",
      "End of epoch 11: running error average = 4.21 running acc average = 0.15\n",
      "End of epoch 12: running error average = 4.00 running acc average = 0.15\n",
      "End of epoch 13: running error average = 3.85 running acc average = 0.15\n",
      "End of epoch 14: running error average = 3.74 running acc average = 0.15\n",
      "End of epoch 15: running error average = 3.58 running acc average = 0.15\n",
      "End of epoch 16: running error average = 3.48 running acc average = 0.15\n",
      "End of epoch 17: running error average = 3.39 running acc average = 0.15\n",
      "End of epoch 18: running error average = 3.29 running acc average = 0.15\n",
      "End of epoch 19: running error average = 3.25 running acc average = 0.15\n",
      "End of epoch 20: running error average = 3.19 running acc average = 0.15\n"
     ]
    }
   ],
   "source": [
    "# 2 hidden layers GD learning rate 10-3\n",
    "n_hidden_1 = 100\n",
    "n_hidden_2 = 100\n",
    "beta = 0.01 #0.009\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([3000, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([10]))\n",
    "}\n",
    "def multilayer_2hidden100(data):\n",
    "    layer_1 = tf.nn.relu(tf.matmul(data, weights['h1']) + biases['b1'])\n",
    "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "out_layer = multilayer_2hidden100(inputs)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "#regularizer = tf.nn.l2_loss(weights)\n",
    "#error = tf.reduce_mean(loss + beta * regularization)\n",
    "\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(out_layer, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(error)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "import data_providers as data_providers\n",
    "train_data = data_providers.MSD10GenreDataProvider('train', batch_size=50, flatten=True, one_hot=True)\n",
    "valid_data = data_providers.MSD10GenreDataProvider('valid', batch_size=50, flatten=True, one_hot=True)\n",
    "num_epoch = 20\n",
    "for e in range(num_epoch):\n",
    "    running_error = 0.\n",
    "    running_acc = 0.\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [train_step, error, accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_acc += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_acc /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e + 1, running_error), \n",
    "          'running acc average = {1:.2f}'.format(e + 1, running_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1: running error average = 1764.26 running acc average = 0.15\n",
      "End of epoch 2: running error average = 924.03 running acc average = 0.20\n",
      "End of epoch 3: running error average = 646.06 running acc average = 0.21\n",
      "End of epoch 4: running error average = 471.24 running acc average = 0.21\n",
      "End of epoch 5: running error average = 352.70 running acc average = 0.21\n",
      "End of epoch 6: running error average = 261.04 running acc average = 0.21\n",
      "End of epoch 7: running error average = 194.71 running acc average = 0.21\n",
      "End of epoch 8: running error average = 144.15 running acc average = 0.21\n",
      "End of epoch 9: running error average = 106.78 running acc average = 0.21\n",
      "End of epoch 10: running error average = 81.91 running acc average = 0.20\n",
      "End of epoch 11: running error average = 63.46 running acc average = 0.21\n",
      "End of epoch 12: running error average = 50.38 running acc average = 0.20\n",
      "End of epoch 13: running error average = 42.30 running acc average = 0.20\n",
      "End of epoch 14: running error average = 37.01 running acc average = 0.19\n",
      "End of epoch 15: running error average = 31.99 running acc average = 0.20\n",
      "End of epoch 16: running error average = 28.84 running acc average = 0.19\n",
      "End of epoch 17: running error average = 26.32 running acc average = 0.19\n",
      "End of epoch 18: running error average = 24.47 running acc average = 0.19\n",
      "End of epoch 19: running error average = 22.44 running acc average = 0.19\n",
      "End of epoch 20: running error average = 21.18 running acc average = 0.18\n"
     ]
    }
   ],
   "source": [
    "# 2 hidden layers GD learning rate 10-4\n",
    "n_hidden_1 = 100\n",
    "n_hidden_2 = 100\n",
    "beta = 0.01 #0.009\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([3000, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([10]))\n",
    "}\n",
    "def multilayer_2hidden100(data):\n",
    "    layer_1 = tf.nn.relu(tf.matmul(data, weights['h1']) + biases['b1'])\n",
    "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "out_layer = multilayer_2hidden100(inputs)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "#regularizer = tf.nn.l2_loss(weights)\n",
    "#error = tf.reduce_mean(loss + beta * regularization)\n",
    "\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(out_layer, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.0001).minimize(error)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "import data_providers as data_providers\n",
    "train_data = data_providers.MSD10GenreDataProvider('train', batch_size=50, flatten=True, one_hot=True)\n",
    "valid_data = data_providers.MSD10GenreDataProvider('valid', batch_size=50, flatten=True, one_hot=True)\n",
    "num_epoch = 20\n",
    "for e in range(num_epoch):\n",
    "    running_error = 0.\n",
    "    running_acc = 0.\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [train_step, error, accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_acc += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_acc /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e + 1, running_error), \n",
    "          'running acc average = {1:.2f}'.format(e + 1, running_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1: running error average = 3307.33 running acc average = 0.11\n",
      "End of epoch 2: running error average = 2221.83 running acc average = 0.12\n",
      "End of epoch 3: running error average = 1954.74 running acc average = 0.14\n",
      "End of epoch 4: running error average = 1778.21 running acc average = 0.15\n",
      "End of epoch 5: running error average = 1653.09 running acc average = 0.15\n",
      "End of epoch 6: running error average = 1523.82 running acc average = 0.16\n",
      "End of epoch 7: running error average = 1420.52 running acc average = 0.17\n",
      "End of epoch 8: running error average = 1333.63 running acc average = 0.17\n",
      "End of epoch 9: running error average = 1244.29 running acc average = 0.17\n",
      "End of epoch 10: running error average = 1182.66 running acc average = 0.18\n",
      "End of epoch 11: running error average = 1122.90 running acc average = 0.18\n",
      "End of epoch 12: running error average = 1065.90 running acc average = 0.19\n",
      "End of epoch 13: running error average = 1008.14 running acc average = 0.19\n",
      "End of epoch 14: running error average = 970.68 running acc average = 0.19\n",
      "End of epoch 15: running error average = 911.00 running acc average = 0.19\n",
      "End of epoch 16: running error average = 886.33 running acc average = 0.19\n",
      "End of epoch 17: running error average = 846.13 running acc average = 0.20\n",
      "End of epoch 18: running error average = 818.00 running acc average = 0.20\n",
      "End of epoch 19: running error average = 781.23 running acc average = 0.20\n",
      "End of epoch 20: running error average = 755.42 running acc average = 0.20\n",
      "End of epoch 21: running error average = 729.91 running acc average = 0.20\n",
      "End of epoch 22: running error average = 703.00 running acc average = 0.20\n",
      "End of epoch 23: running error average = 681.27 running acc average = 0.20\n",
      "End of epoch 24: running error average = 653.64 running acc average = 0.20\n",
      "End of epoch 25: running error average = 630.91 running acc average = 0.20\n",
      "End of epoch 26: running error average = 613.46 running acc average = 0.20\n",
      "End of epoch 27: running error average = 591.80 running acc average = 0.20\n",
      "End of epoch 28: running error average = 574.48 running acc average = 0.20\n",
      "End of epoch 29: running error average = 548.63 running acc average = 0.21\n",
      "End of epoch 30: running error average = 530.30 running acc average = 0.21\n",
      "End of epoch 31: running error average = 521.63 running acc average = 0.21\n",
      "End of epoch 32: running error average = 503.47 running acc average = 0.20\n",
      "End of epoch 33: running error average = 487.20 running acc average = 0.21\n",
      "End of epoch 34: running error average = 472.62 running acc average = 0.20\n",
      "End of epoch 35: running error average = 456.00 running acc average = 0.21\n",
      "End of epoch 36: running error average = 442.81 running acc average = 0.21\n",
      "End of epoch 37: running error average = 428.34 running acc average = 0.21\n",
      "End of epoch 38: running error average = 420.02 running acc average = 0.21\n",
      "End of epoch 39: running error average = 405.82 running acc average = 0.21\n",
      "End of epoch 40: running error average = 394.49 running acc average = 0.21\n",
      "End of epoch 41: running error average = 379.85 running acc average = 0.21\n",
      "End of epoch 42: running error average = 372.83 running acc average = 0.21\n",
      "End of epoch 43: running error average = 358.24 running acc average = 0.21\n",
      "End of epoch 44: running error average = 348.30 running acc average = 0.21\n",
      "End of epoch 45: running error average = 336.27 running acc average = 0.21\n",
      "End of epoch 46: running error average = 327.48 running acc average = 0.21\n",
      "End of epoch 47: running error average = 313.21 running acc average = 0.22\n",
      "End of epoch 48: running error average = 305.39 running acc average = 0.21\n",
      "End of epoch 49: running error average = 299.14 running acc average = 0.21\n",
      "End of epoch 50: running error average = 289.65 running acc average = 0.21\n"
     ]
    }
   ],
   "source": [
    "# 2 hidden layers GD learning rate 10-5\n",
    "n_hidden_1 = 100\n",
    "n_hidden_2 = 100\n",
    "beta = 0.01 #0.009\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([3000, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([10]))\n",
    "}\n",
    "def multilayer_2hidden100(data):\n",
    "    layer_1 = tf.nn.relu(tf.matmul(data, weights['h1']) + biases['b1'])\n",
    "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "out_layer = multilayer_2hidden100(inputs)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "#regularizer = tf.nn.l2_loss(weights)\n",
    "#error = tf.reduce_mean(loss + beta * regularization)\n",
    "\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(out_layer, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.00001).minimize(error)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "import data_providers as data_providers\n",
    "train_data = data_providers.MSD10GenreDataProvider('train', batch_size=50, flatten=True, one_hot=True)\n",
    "valid_data = data_providers.MSD10GenreDataProvider('valid', batch_size=50, flatten=True, one_hot=True)\n",
    "num_epoch = 50\n",
    "for e in range(num_epoch):\n",
    "    running_error = 0.\n",
    "    running_acc = 0.\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [train_step, error, accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_acc += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_acc /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e + 1, running_error), \n",
    "          'running acc average = {1:.2f}'.format(e + 1, running_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1: running error average = 3106.06 running acc average = 0.17\n",
      "End of epoch 2: running error average = 212.59 running acc average = 0.15\n",
      "End of epoch 3: running error average = 29.14 running acc average = 0.13\n",
      "End of epoch 4: running error average = 12.91 running acc average = 0.12\n",
      "End of epoch 5: running error average = 9.02 running acc average = 0.12\n",
      "End of epoch 6: running error average = 6.01 running acc average = 0.12\n",
      "End of epoch 7: running error average = 5.11 running acc average = 0.12\n",
      "End of epoch 8: running error average = 4.50 running acc average = 0.12\n",
      "End of epoch 9: running error average = 4.02 running acc average = 0.12\n",
      "End of epoch 10: running error average = 4.06 running acc average = 0.12\n"
     ]
    }
   ],
   "source": [
    "# 100 hidden units learning rate 10-4\n",
    "n_hidden_1 = 100\n",
    "n_hidden_2 = 100\n",
    "n_hidden_3 = 100\n",
    "beta = 0.01 #0.009\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([3000, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([10]))\n",
    "}\n",
    "def multilayer_3hidden100(data):\n",
    "    layer_1 = tf.nn.relu(tf.matmul(data, weights['h1']) + biases['b1'])\n",
    "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
    "    layer_3 = tf.nn.relu(tf.matmul(layer_2, weights['h3']) + biases['b3'])\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "out_layer = multilayer_3hidden100(inputs)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "#regularizer = tf.nn.l2_loss(weights)\n",
    "#error = tf.reduce_mean(loss + beta * regularization)\n",
    "\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(out_layer, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.0001).minimize(error)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "import data_providers as data_providers\n",
    "train_data = data_providers.MSD10GenreDataProvider('train', batch_size=50, flatten=True, one_hot=True)\n",
    "valid_data = data_providers.MSD10GenreDataProvider('valid', batch_size=50, flatten=True, one_hot=True)\n",
    "num_epoch = 10\n",
    "for e in range(num_epoch):\n",
    "    running_error = 0.\n",
    "    running_acc = 0.\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [train_step, error, accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_acc += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_acc /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e + 1, running_error), \n",
    "          'running acc average = {1:.2f}'.format(e + 1, running_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1: running error average = 9899.99 running acc average = 0.14\n",
      "End of epoch 2: running error average = 5148.98 running acc average = 0.18\n",
      "End of epoch 3: running error average = 3617.99 running acc average = 0.18\n",
      "End of epoch 4: running error average = 2626.01 running acc average = 0.19\n",
      "End of epoch 5: running error average = 1989.34 running acc average = 0.18\n",
      "End of epoch 6: running error average = 1481.91 running acc average = 0.18\n",
      "End of epoch 7: running error average = 1063.26 running acc average = 0.19\n",
      "End of epoch 8: running error average = 742.17 running acc average = 0.18\n",
      "End of epoch 9: running error average = 497.01 running acc average = 0.17\n",
      "End of epoch 10: running error average = 344.01 running acc average = 0.17\n",
      "End of epoch 11: running error average = 268.51 running acc average = 0.16\n",
      "End of epoch 12: running error average = 245.57 running acc average = 0.15\n",
      "End of epoch 13: running error average = 227.41 running acc average = 0.15\n",
      "End of epoch 14: running error average = 204.56 running acc average = 0.15\n",
      "End of epoch 15: running error average = 185.87 running acc average = 0.15\n",
      "End of epoch 16: running error average = 166.74 running acc average = 0.15\n",
      "End of epoch 17: running error average = 147.37 running acc average = 0.15\n",
      "End of epoch 18: running error average = 132.29 running acc average = 0.15\n",
      "End of epoch 19: running error average = 115.07 running acc average = 0.15\n",
      "End of epoch 20: running error average = 103.92 running acc average = 0.15\n",
      "End of epoch 21: running error average = 93.20 running acc average = 0.15\n",
      "End of epoch 22: running error average = 84.26 running acc average = 0.15\n",
      "End of epoch 23: running error average = 75.13 running acc average = 0.14\n",
      "End of epoch 24: running error average = 66.58 running acc average = 0.15\n",
      "End of epoch 25: running error average = 59.51 running acc average = 0.14\n",
      "End of epoch 26: running error average = 54.35 running acc average = 0.15\n",
      "End of epoch 27: running error average = 49.59 running acc average = 0.14\n",
      "End of epoch 28: running error average = 43.85 running acc average = 0.14\n",
      "End of epoch 29: running error average = 39.91 running acc average = 0.15\n",
      "End of epoch 30: running error average = 36.65 running acc average = 0.14\n",
      "End of epoch 31: running error average = 35.45 running acc average = 0.14\n",
      "End of epoch 32: running error average = 31.93 running acc average = 0.14\n",
      "End of epoch 33: running error average = 29.57 running acc average = 0.14\n",
      "End of epoch 34: running error average = 27.42 running acc average = 0.14\n",
      "End of epoch 35: running error average = 25.10 running acc average = 0.15\n",
      "End of epoch 36: running error average = 24.25 running acc average = 0.14\n",
      "End of epoch 37: running error average = 22.53 running acc average = 0.14\n",
      "End of epoch 38: running error average = 20.75 running acc average = 0.14\n",
      "End of epoch 39: running error average = 19.77 running acc average = 0.14\n",
      "End of epoch 40: running error average = 19.02 running acc average = 0.14\n",
      "End of epoch 41: running error average = 17.67 running acc average = 0.14\n",
      "End of epoch 42: running error average = 16.99 running acc average = 0.14\n",
      "End of epoch 43: running error average = 16.09 running acc average = 0.14\n",
      "End of epoch 44: running error average = 15.07 running acc average = 0.14\n",
      "End of epoch 45: running error average = 14.35 running acc average = 0.14\n",
      "End of epoch 46: running error average = 13.21 running acc average = 0.14\n",
      "End of epoch 47: running error average = 13.02 running acc average = 0.14\n",
      "End of epoch 48: running error average = 12.50 running acc average = 0.14\n",
      "End of epoch 49: running error average = 11.84 running acc average = 0.14\n",
      "End of epoch 50: running error average = 11.68 running acc average = 0.14\n"
     ]
    }
   ],
   "source": [
    "# 100 hidden units learning rate 10-5\n",
    "n_hidden_1 = 100\n",
    "n_hidden_2 = 100\n",
    "n_hidden_3 = 100\n",
    "beta = 0.01 #0.009\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([3000, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([10]))\n",
    "}\n",
    "def multilayer_3hidden100(data):\n",
    "    layer_1 = tf.nn.relu(tf.matmul(data, weights['h1']) + biases['b1'])\n",
    "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
    "    layer_3 = tf.nn.relu(tf.matmul(layer_2, weights['h3']) + biases['b3'])\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "out_layer = multilayer_3hidden100(inputs)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "#regularizer = tf.nn.l2_loss(weights)\n",
    "#error = tf.reduce_mean(loss + beta * regularization)\n",
    "\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(out_layer, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.00001).minimize(error)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "import data_providers as data_providers\n",
    "train_data = data_providers.MSD10GenreDataProvider('train', batch_size=50, flatten=True, one_hot=True)\n",
    "valid_data = data_providers.MSD10GenreDataProvider('valid', batch_size=50, flatten=True, one_hot=True)\n",
    "num_epoch = 50\n",
    "for e in range(num_epoch):\n",
    "    running_error = 0.\n",
    "    running_acc = 0.\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [train_step, error, accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_acc += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_acc /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e + 1, running_error), \n",
    "          'running acc average = {1:.2f}'.format(e + 1, running_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1: running error average = 5926.74 running acc average = 0.19\n",
      "End of epoch 2: running error average = 64.26 running acc average = 0.14\n",
      "End of epoch 3: running error average = 21.80 running acc average = 0.12\n",
      "End of epoch 4: running error average = 11.23 running acc average = 0.12\n",
      "End of epoch 5: running error average = 7.85 running acc average = 0.12\n",
      "End of epoch 6: running error average = 6.42 running acc average = 0.12\n",
      "End of epoch 7: running error average = 5.28 running acc average = 0.11\n",
      "End of epoch 8: running error average = 5.11 running acc average = 0.11\n",
      "End of epoch 9: running error average = 4.42 running acc average = 0.11\n",
      "End of epoch 10: running error average = 3.88 running acc average = 0.11\n"
     ]
    }
   ],
   "source": [
    "# 200 hidden units learning rate 10-4\n",
    "n_hidden_1 = 200\n",
    "n_hidden_2 = 200\n",
    "n_hidden_3 = 200\n",
    "beta = 0.01 #0.009\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([3000, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([10]))\n",
    "}\n",
    "def multilayer_3hidden200(data):\n",
    "    layer_1 = tf.nn.relu(tf.matmul(data, weights['h1']) + biases['b1'])\n",
    "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
    "    layer_3 = tf.nn.relu(tf.matmul(layer_2, weights['h3']) + biases['b3'])\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "out_layer = multilayer_3hidden200(inputs)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "#regularizer = tf.nn.l2_loss(weights)\n",
    "#error = tf.reduce_mean(loss + beta * regularization)\n",
    "\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(out_layer, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.0001).minimize(error)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "import data_providers as data_providers\n",
    "train_data = data_providers.MSD10GenreDataProvider('train', batch_size=50, flatten=True, one_hot=True)\n",
    "valid_data = data_providers.MSD10GenreDataProvider('valid', batch_size=50, flatten=True, one_hot=True)\n",
    "num_epoch = 10\n",
    "for e in range(num_epoch):\n",
    "    running_error = 0.\n",
    "    running_acc = 0.\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [train_step, error, accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_acc += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_acc /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e + 1, running_error), \n",
    "          'running acc average = {1:.2f}'.format(e + 1, running_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1: running error average = 23589.52 running acc average = 0.18\n",
      "End of epoch 2: running error average = 11705.73 running acc average = 0.23\n",
      "End of epoch 3: running error average = 7856.59 running acc average = 0.23\n",
      "End of epoch 4: running error average = 5513.54 running acc average = 0.23\n",
      "End of epoch 5: running error average = 3802.50 running acc average = 0.23\n",
      "End of epoch 6: running error average = 2608.29 running acc average = 0.23\n",
      "End of epoch 7: running error average = 1848.06 running acc average = 0.22\n",
      "End of epoch 8: running error average = 1423.08 running acc average = 0.21\n",
      "End of epoch 9: running error average = 1132.32 running acc average = 0.21\n",
      "End of epoch 10: running error average = 882.90 running acc average = 0.20\n"
     ]
    }
   ],
   "source": [
    "# 200 hidden units learning rate 10-5\n",
    "n_hidden_1 = 200\n",
    "n_hidden_2 = 200\n",
    "n_hidden_3 = 200\n",
    "beta = 0.01 #0.009\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([3000, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([10]))\n",
    "}\n",
    "def multilayer_3hidden200(data):\n",
    "    layer_1 = tf.nn.relu(tf.matmul(data, weights['h1']) + biases['b1'])\n",
    "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
    "    layer_3 = tf.nn.relu(tf.matmul(layer_2, weights['h3']) + biases['b3'])\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "out_layer = multilayer_3hidden200(inputs)\n",
    "\n",
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)\n",
    "#regularizer = tf.nn.l2_loss(weights)\n",
    "#error = tf.reduce_mean(loss + beta * regularization)\n",
    "\n",
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(out_layer, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.00001).minimize(error)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "import data_providers as data_providers\n",
    "train_data = data_providers.MSD10GenreDataProvider('train', batch_size=50, flatten=True, one_hot=True)\n",
    "valid_data = data_providers.MSD10GenreDataProvider('valid', batch_size=50, flatten=True, one_hot=True)\n",
    "num_epoch = 10\n",
    "for e in range(num_epoch):\n",
    "    running_error = 0.\n",
    "    running_acc = 0.\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [train_step, error, accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_acc += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_acc /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e + 1, running_error), \n",
    "          'running acc average = {1:.2f}'.format(e + 1, running_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34000, 120, 25), (34000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.load('/afs/inf.ed.ac.uk/user/s17/s1773005/mlpractical/data/inputs.npy')\n",
    "b = np.load('/afs/inf.ed.ac.uk/user/s17/s1773005/mlpractical/data/targets.npy')\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/afs/inf.ed.ac.uk/user/s17/s1773005/mlpractical/data/inputs.npy'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.join(\n",
    "            os.environ['MLP_DATA_DIR'], 'inputs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(b == ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
